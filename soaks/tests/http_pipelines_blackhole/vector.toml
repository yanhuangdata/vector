data_dir = "/var/lib/vector"

##
## Sources
##

[sources.internal_metrics]
type = "internal_metrics"

[sources.logs]
type = "http"
address = "0.0.0.0:8282"
encoding = "text"

##
## Transforms
##

[transforms.preprocessing]
type = "remap"
inputs = ["logs"]
source = '''
., err = parse_json(.message)
if starts_with(strip_whitespace!(.message), "{") {
  custom, err = parse_json(.message)
  if (err == null) {
    .custom, err = compact(custom, string: false)
  }
}
.custom.message = .message
.tags = []
'''

[transforms.processing]
type = "pipelines"
inputs = ["preprocessing"]

[[transforms.processing.logs]]
name = "nginx"
filter.type = "datadog_search"
filter.source = "source:nginx"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
    "(?s)%{access.common} (%{number:duration:scale(1000000000)} )?\"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
    "(?s)%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data::keyvalue(\": \",\",\")})?",
  ],
  aliases: {
    "access.common": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
    "access.combined": "%{access.common} (%{number:duration:scale(1000000000)} )?\"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
    "error.format": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data::keyvalue(\": \",\",\")})?",
    "_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
    "_bytes_written": "%{integer:network.bytes_written}",
    "_client_ip": "%{ipOrHost:network.client.ip}",
    "_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
    "_url": "%{notSpace:http.url}",
    "_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
    "_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
    "_referer": "%{notSpace:http.referer}",
    "_status_code": "%{integer:http.status_code}",
    "_method": "%{word:http.method}",
    "_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}",
    "_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.client; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.client)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.request,
  patterns: [
    "(?s)(?>%{_method} |)%{_url}(?> %{_version}|)",
  ],
  aliases: {
    "request_parsing": "(?>%{_method} |)%{_url}(?> %{_version}|)",
    "_method": "%{word:http.method}",
    "_url": "%{notSpace:http.url}",
    "_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  details.queryString = del(details.query)
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent, mode: "fast"); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
  .timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
  .custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
  .custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
  .custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
  .custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "redis"
filter.type = "datadog_search"
filter.source = "source:redis"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_pid}:%{_role} %{_date} %{_severity} %{data:message}",
  ],
  aliases: {
    "default_format": "%{_pid}:%{_role} %{_date} %{_severity} %{data:message}",
    "_date": "(%{date(\"dd MMM HH:mm:ss.SSS\"):date}|%{date(\"dd MMM yyyy HH:mm:ss.SSS\"):date})",
    "_pid": "%{integer:pid}",
    "_severity": "%{notSpace:severity}",
    "_role": "%{word:role}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
  .timestamp = .custom.date
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@severity: \".\"") {
  .custom.redis.severity = "debug"
} else if match_datadog_query(., "@severity: \"-\"") {
  .custom.redis.severity = "verbose"
} else if match_datadog_query(., "@severity: \"*\"") {
  .custom.redis.severity = "notice"
} else if match_datadog_query(., "@severity: \"#\"") {
  .custom.redis.severity = "warning"
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.redis.severity) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "consul"
filter.type = "datadog_search"
filter.source = "source:consul"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)==>\\s+(?>%{_status}:|)%{data:message}",
    "(?s)(\\s*|\\t*)%{_date_2} %{_hostname} %{_app}\\[%{_thread_id}]:\\s+%{data:message}",
    "(?s)(\\s*|\\t*)(%{_date}|%{_date_3}) \\[%{_status}\\] %{_app}:( %{_event}:)?%{data:message}",
  ],
  aliases: {
    "Upstart_format": "==>\\s+(?>%{_status}:|)%{data:message}",
    "Consul_template": "(\\s*|\\t*)%{_date_2} %{_hostname} %{_app}\\[%{_thread_id}]:\\s+%{data:message}",
    "Default_format": "(\\s*|\\t*)(%{_date}|%{_date_3}) \\[%{_status}\\] %{_app}:( %{_event}:)?%{data:message}",
    "_date": "%{date(\"yyyy/MM/dd HH:mm:ss\"):timestamp}",
    "_date_2": "%{date(\"MMM dd HH:mm:ss\"):timestamp}",
    "_date_3": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZZ\"):timestamp}",
    "_status": "%{word:level}",
    "_app": "%{data:app}",
    "_event": "%{word:event}",
    "_hostname": "%{notSpace:hostname}",
    "_thread_id": "%{integer:logger.thread.id}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "python"
filter.type = "datadog_search"
filter.source = "source:python"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)(%{_python_prefix}|%{_datadog_prefix})\\s+%{data::keyvalue}Traceback \\(most recent call last\\):\\s*%{data:error.stack}",
    "(?s)(%{_python_prefix}|%{_datadog_prefix})\\s+%{data::keyvalue}",
    "(?s)%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+%{data::keyvalue}((\\n|\\t)%{data:error.stack})?",
  ],
  aliases: {
    "traceback_format": "(%{_python_prefix}|%{_datadog_prefix})\\s+%{data::keyvalue}Traceback \\(most recent call last\\):\\s*%{data:error.stack}",
    "python_format": "(%{_python_prefix}|%{_datadog_prefix})\\s+%{data::keyvalue}",
    "python_fallback": "%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+%{data::keyvalue}((\\n|\\t)%{data:error.stack})?",
    "_datadog_prefix": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp} %{word:levelname}\\s+\\[%{notSpace:process.name}\\]\\s+\\[%{notSpace:filename}:%{number:lineno}\\]\\s+\\[%{data::keyvalue}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\] -",
    "_python_prefix": "%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+\\[%{notSpace:process.name}\\]\\s+\\[%{integer:process.id}\\]"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.levelname) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.error.stack == null {
  if (source = .custom.traceback; source != null) {
    .custom.error.stack = source
  }
}
del(.custom.traceback)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.logger.name == null {
  if (source = .custom.name; source != null) {
    .custom.logger.name = source
  }
}
del(.custom.name)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.logger.thread_name == null {
  if (source = .custom.threadName; source != null) {
    .custom.logger.thread_name = source
  }
}
del(.custom.threadName)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.error.stack,
  patterns: [
    "(?s)File \"%{notSpace:filename}\", line %{integer:lineno}.*\\s+%{regex(\"[a-zA-Z]*Error[a-zA-Z]*\"):error.kind}: %{data:error.message}",
  ],
  aliases: {
    "parsing_traceback": "File \"%{notSpace:filename}\", line %{integer:lineno}.*\\s+%{regex(\"[a-zA-Z]*Error[a-zA-Z]*\"):error.kind}: %{data:error.message}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
  .trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.dd.env != null {
  if (tag, err = "env:" + to_string(.custom.dd.env); err == null) {
    .tags, err = push(.tags, tag)
  }
}
del(.custom.dd.env)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.dd.version != null {
  if (tag, err = "version:" + to_string(.custom.dd.version); err == null) {
    .tags, err = push(.tags, tag)
  }
}
del(.custom.dd.version)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
  .service = .custom.dd.service
}
'''

[[transforms.processing.logs]]
name = "rabbitmq"
filter.type = "datadog_search"
filter.source = "source:rabbitmq"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)=%{_status} REPORT(====)? (%{_date_d}|%{_date_dd}) =(==)?\\s*%{data:message}",
    "(?s)%{_date_latest}\\s\\[%{_status}\\]\\s*%{data:message}",
  ],
  aliases: {
    "rabbit_default": "=%{_status} REPORT(====)? (%{_date_d}|%{_date_dd}) =(==)?\\s*%{data:message}",
    "rabbit_latest": "%{_date_latest}\\s\\[%{_status}\\]\\s*%{data:message}",
    "_date_d": "%{date(\"d-MMM-yyyy::HH:mm:ss\"):timestamp}",
    "_date_dd": "%{date(\"dd-MMM-yyyy::HH:mm:ss\"):timestamp}",
    "_date_latest": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):timestamp}",
    "_status": "%{word:status}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
  .message = .custom.message
}
'''

[[transforms.processing.logs]]
name = "zookeeper"
filter.type = "datadog_search"
filter.source = "source:zookeeper"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?",
    "(?s)%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?",
  ],
  aliases: {
    "Zookeeper_default": "(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?",
    "Zookeeper_recommended": "%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?",
    "_date": "%{date(\"yyyy-MM-dd HH:mm:ss\"):timestamp}",
    "_date_ms": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
    "_duration": "%{integer:duration}",
    "_thread_name": "%{notSpace:logger.thread_name}",
    "_status": "%{word:status}",
    "_logger_name": "%{notSpace:logger.name}",
    "_context": "%{notSpace:logger.context}",
    "_line": "%{integer:line}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.error.stack,
  patterns: [
    "(?s)%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*",
  ],
  aliases: {
    "error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
  .message = .custom.msg
}
'''

[[transforms.processing.logs]]
name = "elasticsearch"
filter.type = "datadog_search"
filter.source = "source:elasticsearch"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.search.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\]\\[%{_shard}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*",
    "(?s)\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.indexing.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*",
    "(?s)\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[%{_logger}\\s*\\]\\s*(\\[%{_node}\\])?.*",
  ],
  aliases: {
    "Elasticsearch_search_query": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.search.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\]\\[%{_shard}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*",
    "Elasticsearch_slow_indexing": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.indexing.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*",
    "Elasticsearch_default": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[%{_logger}\\s*\\]\\s*(\\[%{_node}\\])?.*",
    "_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss,SSS\"):timestamp}",
    "_date_format2": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
    "_status": "%{word:level}",
    "_operation": "%{notSpace:elasticsearch.operation}",
    "_node": "%{hostname:nodeId}",
    "_index": "%{notSpace:elasticsearch.index}",
    "_shard": "%{integer:elasticsearch.shard}",
    "_duration": "%{integer:duration:scale(1000000)}",
    "_logger": "%{notSpace:logger.name}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.node_name == null {
  if (source = .custom.nodeId; source != null) {
    .custom.node_name = source
  }
}
'''

[[transforms.processing.logs]]
name = "kafka"
filter.type = "datadog_search"
filter.source = "source:kafka"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?",
    "(?s)%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?",
    "(?s)\\[%{_date_ms}\\] %{_status} %{data:msg} \\(%{_logger_name}\\)",
  ],
  aliases: {
    "Kafka_default": "(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?",
    "Kafka_recommended": "%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?",
    "Kafka_standard": "\\[%{_date_ms}\\] %{_status} %{data:msg} \\(%{_logger_name}\\)",
    "_date": "%{date(\"yyyy-MM-dd HH:mm:ss\"):timestamp}",
    "_date_ms": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
    "_duration": "%{integer:duration}",
    "_thread_name": "%{notSpace:logger.thread_name}",
    "_status": "%{word:status}",
    "_logger_name": "%{notSpace:logger.name}",
    "_context": "%{notSpace:logger.context}",
    "_line": "%{integer:line}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.error.stack,
  patterns: [
    "(?s)%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*",
  ],
  aliases: {
    "error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
  .message = .custom.msg
}
'''

[[transforms.processing.logs]]
name = "couchdb"
filter.type = "datadog_search"
filter.source = "source:couchdb"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)\\[%{_date_access}\\] \\[%{_level}\\] %{notSpace} %{_client_ip} - - %{_method} %{_url} %{_status_code}.*",
    "(?s)\\[%{_level}\\] %{_date} %{_user}@%{_client_ip} %{notSpace} (-------- CRASH REPORT\\s*%{data:error.stack}|%{data::keyvalue(\":\")})",
    "(?s)%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date_access} \\[%{_level}\\] .*",
  ],
  aliases: {
    "http_rule": "\\[%{_date_access}\\] \\[%{_level}\\] %{notSpace} %{_client_ip} - - %{_method} %{_url} %{_status_code}.*",
    "default_format": "\\[%{_level}\\] %{_date} %{_user}@%{_client_ip} %{notSpace} (-------- CRASH REPORT\\s*%{data:error.stack}|%{data::keyvalue(\":\")})",
    "fallback_format": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date_access} \\[%{_level}\\] .*",
    "_date_access": "(%{date(\"EEE, dd MMM yyyy HH:mm:ss z\"):date_access}|%{date(\"EEE, d MMM yyyy HH:mm:ss z\"):date_access})",
    "_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ\"):date_access}",
    "_level": "%{word:level}",
    "_method": "%{word:http.method}",
    "_url": "%{notSpace:http.url}",
    "_status_code": "%{number:http.status_code}",
    "_client_ip": "%{notSpace:network.client.ip}",
    "_user": "%{notSpace:db.user}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
  .timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  details.queryString = del(details.query)
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs]]
name = "docker"
filter.type = "datadog_search"
filter.source = "source:docker"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_level}%{_kube_date}\\s+%{_thread_id} %{_logger_name}:%{_line}\\].*",
    "(?s)%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
    "(?s)(?>\\[ %{word:process.name} \\] )?%{_dd_date} \\| %{word:level} \\| \\(%{_logger_name}:%{_line} in %{word:datadog.process}\\) \\| .*",
  ],
  aliases: {
    "Kubernetes_format": "%{_level}%{_kube_date}\\s+%{_thread_id} %{_logger_name}:%{_line}\\].*",
    "access.combined": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
    "datadog_format": "(?>\\[ %{word:process.name} \\] )?%{_dd_date} \\| %{word:level} \\| \\(%{_logger_name}:%{_line} in %{word:datadog.process}\\) \\| .*",
    "_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
    "_bytes_written": "%{integer:network.bytes_written}",
    "_client_ip": "%{ipOrHost:network.client.ip}",
    "_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
    "_url": "%{notSpace:http.url}",
    "_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
    "_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
    "_referer": "%{notSpace:http.referer}",
    "_status_code": "%{integer:http.status_code}",
    "_method": "%{word:http.method}",
    "_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):timestamp}",
    "_dd_date": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp}",
    "_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}",
    "_level": "%{regex(\"[\\\\w]\"):level}",
    "_kube_date": "%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}",
    "_thread_id": "%{number:logger.thread_id}",
    "_logger_name": "%{notSpace:logger.name}",
    "_line": "%{number:lineno}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "datadog_agent"
filter.type = "datadog_search"
filter.source = "source:(agent OR datadog-agent OR datadog-agent-cluster-worker OR datadog-cluster-agent OR process-agent OR security-agent OR system-probe OR trace-agent)"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\) \\|( %{data::keyvalue(\":\")} \\|)?( - \\|)?( \\(%{notSpace:pyFilename}:%{number:pyLineno}\\) \\|)?%{data}",
    "(?s)%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\)%{data}",
    "(?s)%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level}\\s+\\| %{word:class} \\| %{data}",
  ],
  aliases: {
    "agent_rule": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\) \\|( %{data::keyvalue(\":\")} \\|)?( - \\|)?( \\(%{notSpace:pyFilename}:%{number:pyLineno}\\) \\|)?%{data}",
    "agent_rule_pre_611": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\)%{data}",
    "jmxfetch_rule": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level}\\s+\\| %{word:class} \\| %{data}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "ruby"
filter.type = "datadog_search"
filter.source = "source:ruby"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.logger.name == null {
  if (source = .custom.logger; source != null) {
    .custom.logger.name = source
  }
}
del(.custom.logger)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.logger.thread_name == null {
  if (source = .custom.thread; source != null) {
    .custom.logger.thread_name = source
  }
}
del(.custom.thread)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.error.stack == null {
  if (source = .custom.exception; source != null) {
    .custom.error.stack = source
  } else if (source = .custom.stack_trace; source != null) {
    .custom.error.stack = source
  }
}
del(.custom.exception)
del(.custom.stack_trace)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{integer:http.status_code}(?: (?:%{word}|Internal Server Error))? in %{number:duration}ms \\((?>Views: %{number:views}ms \\| )?ActiveRecord: %{number:activerecord}ms\\)",
    "(?s)(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} by %{notSpace:processor}.*",
    "(?s)(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{word:http.method} \"%{notSpace:http.url_details.path}\" for %{ipOrHost:network.client.ip}.*",
    "(?s)%{_datadog_prefix} %{data::keyvalue(\"=\",\"/\")}(?>(\\n|\\t)%{data:error.stack})?",
    "(?s)(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?Received: %{data::json}",
    "(?s)%{_ruby_log_prefix}\\s+%{data:error.message}(?>(\\n|\\t)%{data:error.stack})?",
    "(?s)%{data::keyvalue(\"=\",\"/\")}",
  ],
  aliases: {
    "completed_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{integer:http.status_code}(?: (?:%{word}|Internal Server Error))? in %{number:duration}ms \\((?>Views: %{number:views}ms \\| )?ActiveRecord: %{number:activerecord}ms\\)",
    "processing_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} by %{notSpace:processor}.*",
    "started_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{word:http.method} \"%{notSpace:http.url_details.path}\" for %{ipOrHost:network.client.ip}.*",
    "datadog_format": "%{_datadog_prefix} %{data::keyvalue(\"=\",\"/\")}(?>(\\n|\\t)%{data:error.stack})?",
    "received_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?Received: %{data::json}",
    "Ruby_default": "%{_ruby_log_prefix}\\s+%{data:error.message}(?>(\\n|\\t)%{data:error.stack})?",
    "Ruby_keyvalue": "%{data::keyvalue(\"=\",\"/\")}",
    "_date": "(?:%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\"):date}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):date})",
    "_status": "%{word:level}",
    "_thread_id": "%{word:logger.thread_id}",
    "_thread_name": "%{notSpace:logger.thread_name}",
    "_logger_name": "%{notSpace:logger.name}",
    "_ruby_log_prefix": "%{word}, \\[%{_date} #%{_thread_id}\\]\\s+%{_status}\\s+--\\s+(?:%{_logger_name})?:(?:\\s+%{_trace_rule})?",
    "_trace_rule": "\\[%{data::keyvalue}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]",
    "_datadog_prefix": "\\[%{date(\"yyyy-MM-dd HH:mm:ss Z\"):date}\\]\\[%{word:application}\\]\\[%{_status}\\]%{_trace_rule}",
    "_activate_status": "%{word:active_directory.process_status}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
  .timestamp = .custom.date
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.error.stack,
  patterns: [
    "(?s)%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*",
  ],
  aliases: {
    "error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.http.method == null {
  if (source = .custom.method; source != null) {
    .custom.http.method = source
  }
}
del(.custom.method)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.http.url_details.path == null {
  if (source = .custom.path; source != null) {
    .custom.http.url_details.path = source
  }
}
del(.custom.path)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.http.status_code == null {
  if (source = .custom.status; source != null) {
    .custom.http.status_code = source
  }
}
del(.custom.status)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
  .trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.dd.env != null {
  if (tag, err = "env:" + to_string(.custom.dd.env); err == null) {
    .tags, err = push(.tags, tag)
  }
}
del(.custom.dd.env)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.dd.version != null {
  if (tag, err = "version:" + to_string(.custom.dd.version); err == null) {
    .tags, err = push(.tags, tag)
  }
}
del(.custom.dd.version)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
  .service = .custom.dd.service
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.duration*1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs]]
name = "vault"
filter.type = "datadog_search"
filter.source = "source:vault"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_date}\\s+\\[%{_level}\\]\\s+%{notSpace:vault.service}:\\s+%{data:message}",
    "(?s)%{_date}\\s+\\[%{_level}\\]\\s+%{data:message}",
  ],
  aliases: {
    "vault_server_svc": "%{_date}\\s+\\[%{_level}\\]\\s+%{notSpace:vault.service}:\\s+%{data:message}",
    "vault_server": "%{_date}\\s+\\[%{_level}\\]\\s+%{data:message}",
    "_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp}",
    "_level": "%{word:level}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
  .message = .custom.message
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.id == null {
  if (source = .custom.auth.display_name; source != null) {
    .custom.usr.id = source
  } else if (source = .custom.auth.metatdata.username; source != null) {
    .custom.usr.id = source
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.http.url_details.path == null {
  if (source = .custom.request.path; source != null) {
    .custom.http.url_details.path = source
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.http.status_code == null {
  if (source = .custom.request.data.http_status_code; source != null) {
    .custom.http.status_code = source
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.request.remote_address; source != null) {
    .custom.network.client.ip = source
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.http.method == null {
  if (source = .custom.request.operation; source != null) {
    .custom.http.method = source
  }
}
'''

[[transforms.processing.logs]]
name = "nginx_ingress_controller"
filter.type = "datadog_search"
filter.source = "source:nginx-ingress-controller"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_client_ip}(?: - \\[%{notSpace}\\])? - %{_ident} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\" %{_request_size} %{_duration} \\[%{_proxy_name}\\](?: \\[%{_alternate_proxy_name}?\\])? (?:%{_upstream_ip}:%{_upstream_port}|-)(?:, %{notSpace})?(?:, %{notSpace})? (?:%{_bytes_read}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_time}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_status}|-)(?:, %{number}|, -)?(?:, %{number}|, -)?(?: %{_request_id})?.*",
    "(?s)%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data::keyvalue(\": \",\",\")})?",
    "(?s)%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):date_access}\\s+%{number} %{notSpace:logger.name}:%{number:lineno}\\] .*",
  ],
  aliases: {
    "access.common": "%{_client_ip}(?: - \\[%{notSpace}\\])? - %{_ident} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\" %{_request_size} %{_duration} \\[%{_proxy_name}\\](?: \\[%{_alternate_proxy_name}?\\])? (?:%{_upstream_ip}:%{_upstream_port}|-)(?:, %{notSpace})?(?:, %{notSpace})? (?:%{_bytes_read}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_time}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_status}|-)(?:, %{number}|, -)?(?:, %{number}|, -)?(?: %{_request_id})?.*",
    "error.format": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data::keyvalue(\": \",\",\")})?",
    "controller_format": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):date_access}\\s+%{number} %{notSpace:logger.name}:%{number:lineno}\\] .*",
    "_request_id": "%{notSpace:http.request_id}",
    "_upstream_status": "%{number:http.upstream_status_code}",
    "_upstream_time": "%{number:http.upstream_duration}",
    "_bytes_read": "%{number:network.bytes_read}",
    "_upstream_port": "%{number:network.destination.port}",
    "_upstream_ip": "%{ipOrHost:network.destination.ip}",
    "_proxy_name": "%{notSpace:proxy.name}",
    "_alternate_proxy_name": "%{notSpace:proxy.alternate_name}",
    "_duration": "%{number:duration:scale(1000000000)}",
    "_request_size": "%{number:network.request_size}",
    "_bytes_written": "%{integer:network.bytes_written}",
    "_client_ip": "%{ipOrHost:network.client.ip}",
    "_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
    "_url": "%{notSpace:http.url}",
    "_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
    "_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
    "_referer": "%{notSpace:http.referer}",
    "_status_code": "%{integer:http.status_code}",
    "_method": "%{word:http.method}",
    "_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}",
    "_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.client; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.client)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.request,
  patterns: [
    "(?s)(?>%{_method} |)%{_url}(?> %{_version}|)",
  ],
  aliases: {
    "request_parsing": "(?>%{_method} |)%{_url}(?> %{_version}|)",
    "_method": "%{word:http.method}",
    "_url": "%{notSpace:http.url}",
    "_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  details.queryString = del(details.query)
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent, mode: "fast"); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
  .timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
  .custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
  .custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
  .custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
  .custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "mysql"
filter.type = "datadog_search"
filter.source = "source:mysql"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)(%{_timestamp}\\s+|(?:%{_rawtimestamp}\\s+)?)%{integer:thread_id}\\s+%{_operation}\\s+%{_raw_query}",
    "(?s)(%{_timestamp}|%{_timestamp_mariadb_post_1015}) %{integer:thread_id} \\[%{_severity}\\] %{data:message}",
    "(?s)%{_rawtimestamp}\\s+(?>InnoDB:|\\[%{_severity}\\])\\s+%{data:message}",
    "(?s)(%{_time_line}\\s)?(%{_user_line}\\s)?\\# %{data::keyvalue(\": \")}(%{_instance_line}\\s)?%{_set_line}\\s%{_query_line}",
  ],
  aliases: {
    "query_format": "(%{_timestamp}\\s+|(?:%{_rawtimestamp}\\s+)?)%{integer:thread_id}\\s+%{_operation}\\s+%{_raw_query}",
    "default_format": "(%{_timestamp}|%{_timestamp_mariadb_post_1015}) %{integer:thread_id} \\[%{_severity}\\] %{data:message}",
    "raw_default_format": "%{_rawtimestamp}\\s+(?>InnoDB:|\\[%{_severity}\\])\\s+%{data:message}",
    "slow_query_format": "(%{_time_line}\\s)?(%{_user_line}\\s)?\\# %{data::keyvalue(\": \")}(%{_instance_line}\\s)?%{_set_line}\\s%{_query_line}",
    "_timestamp": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ\"):db.date}",
    "_timestamp_mariadb_post_1015": "(%{date(\"yyyy-MM-dd HH:mm:ss\"):db.date}|%{date(\"yyyy-MM-dd  H:mm:ss\"):db.date})",
    "_rawtimestamp": "%{date(\"yyMMdd HH:mm:ss\"):db.date}",
    "_severity": "%{notSpace:db.severity}",
    "_client_ip": "%{ipOrHost:network.client.ip}",
    "_client_port": "%{integer:network.client.port}",
    "_operation": "%{word:db.operation}",
    "_database": "%{word:db.instance}",
    "_raw_query": "%{data:db.statement}",
    "_time_line": "\\# Time: %{notSpace}(\\s+%{notSpace})?",
    "_user_line": "\\# User@Host\\: %{notSpace:db.user}\\s+@\\s+%{notSpace:db.host}\\s+\\[(%{notSpace:network.client.ip})?\\](\\s+Id\\:\\s+%{number:mysql.query.id})?",
    "_instance_line": "use %{notSpace:db.instance};",
    "_set_line": "SET timestamp=%{number:mysql.query.timestamp};",
    "_query_line": "%{data:db.slow_statement}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.db.slow_statement,
  patterns: [
    "(?s)%{word:db.operation} .*",
  ],
  aliases: {
    "slow_query_format": "%{word:db.operation} .*"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.db.statement == null {
  if (source = .custom.db.slow_statement; source != null) {
    .custom.db.statement = source
  }
}
del(.custom.db.slow_statement)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.Query_time * 1000000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.mysql.query.timestamp * 1000; err == null) {
  .custom.db.date = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.bytes_written == null {
  if (source = .custom.Bytes_sent; source != null) {
    .custom.network.bytes_written = source
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.bytes_read == null {
  if (source = .custom.Bytes_received; source != null) {
    .custom.network.bytes_read = source
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
  .timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "kubernetes_cluster_autoscaler"
filter.type = "datadog_search"
filter.source = "source:cluster-autoscaler"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}",
  ],
  aliases: {
    "cluster_scheduler": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
  .message = .custom.msg
}
'''

[[transforms.processing.logs]]
name = "aws_alb_ingress_controller"
filter.type = "datadog_search"
filter.source = "source:aws-alb-ingress-controller"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}",
  ],
  aliases: {
    "aws_alb_ingress_controller": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
  .message = .custom.msg
}
'''

[[transforms.processing.logs]]
name = "proxysql"
filter.type = "datadog_search"
filter.source = "source:proxysql"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_longDate}\\s+%{_logger}\\:\\s+%{_level}\\s+%{data:message}",
    "(?s)%{_longDate}\\s+%{_level}\\s+%{data:message}",
    "(?s)%{_longDate}.*%{_level}%{data:message}",
    "(?s)%{data:proxysql_service}\\s+%{_revision}\\s+--\\s+%{notSpace:logger.name}\\s+--\\s+%{_textDate}",
  ],
  aliases: {
    "extendedFormat": "%{_longDate}\\s+%{_logger}\\:\\s+%{_level}\\s+%{data:message}",
    "simplifiedFormat": "%{_longDate}\\s+%{_level}\\s+%{data:message}",
    "safeGuard": "%{_longDate}.*%{_level}%{data:message}",
    "stdout": "%{data:proxysql_service}\\s+%{_revision}\\s+--\\s+%{notSpace:logger.name}\\s+--\\s+%{_textDate}",
    "_longDate": "%{date(\"yyyy-MM-dd HH:mm:ss\"):date}",
    "_textDate": "%{date(\"EEE MMM dd HH:mm:ss yyyy\"):date}",
    "_level": "\\[%{word:level}\\]",
    "_logger": "%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}\\(.*\\)",
    "_revision": "rev.\\s+%{regex(\"\\\\d+.\\\\d+.\\\\d+\"):revision}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.duration,
  patterns: [
    "(?s)%{number:duration:scale(1000000)}ms",
    "(?s)%{number:duration:scale(1000000000)}s",
    "(?s)%{number:duration:scale(1000)}us",
    "(?s)%{number:duration:scale(1)}ns",
  ],
  aliases: {
    "duration_ms": "%{number:duration:scale(1000000)}ms",
    "duration_s": "%{number:duration:scale(1000000000)}s",
    "duration_us": "%{number:duration:scale(1000)}us",
    "duration_ns": "%{number:duration:scale(1)}ns"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.client_addr,
  patterns: [
    "(?s)%{ipOrHost:network.client.ip}:%{number:network.client.port}",
  ],
  aliases: {
    "ip_and_port": "%{ipOrHost:network.client.ip}:%{number:network.client.port}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.proxy_addr,
  patterns: [
    "(?s)%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}",
  ],
  aliases: {
    "ip_and_port": "%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.extra_info,
  patterns: [
    "(?s)%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}",
  ],
  aliases: {
    "logger": "%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.db.instance == null {
  if (source = .custom.schemaname; source != null) {
    .custom.db.instance = source
  }
}
del(.custom.schemaname)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.db.user == null {
  if (source = .custom.username; source != null) {
    .custom.db.user = source
  }
}
del(.custom.username)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.logger.thread_id == null {
  if (source = .custom.thread_id; source != null) {
    .custom.logger.thread_id = source
  }
}
del(.custom.thread_id)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.client,
  patterns: [
    "(?s)%{ipOrHost:network.client.ip}:%{number:network.client.port}",
  ],
  aliases: {
    "ip_and_port": "%{ipOrHost:network.client.ip}:%{number:network.client.port}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.duration_us * 1000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.db.statement == null {
  if (source = .custom.query; source != null) {
    .custom.db.statement = source
  }
}
del(.custom.query)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.db.rows_affected == null {
  if (source = .custom.rows_affected; source != null) {
    .custom.db.rows_affected = source
  }
}
del(.custom.rows_affected)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.db.rows_sent == null {
  if (source = .custom.rows_sent; source != null) {
    .custom.db.rows_sent = source
  }
}
del(.custom.rows_sent)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.server,
  patterns: [
    "(?s)%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}",
  ],
  aliases: {
    "ip_and_port": "%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
  .timestamp = .custom.date
} else if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
} else if exists(.custom.endtime_timestamp_us) {
  .timestamp = .custom.endtime_timestamp_us
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
  .message = .custom.message
}
'''

[[transforms.processing.logs]]
name = "azure"
filter.type = "datadog_search"
filter.source = "source:(azure OR azure.alertsmanagement OR azure.analysisservices OR azure.apiconfiguration OR azure.apimanagement OR azure.authorization OR azure.automation OR azure.batchai OR azure.batchazure.cache OR azure.blockchain OR azure.cache OR azure.cdn OR azure.classiccompute OR azure.classicstorage OR azure.cognitiveservices OR azure.containerinstance OR azure.containerregistry OR azure.containerservice OR azure.datafactory OR azure.datalakestore OR azure.dbformariadb OR azure.dbformysql OR azure.dbforpostgresql OR azure.devices OR azure.documentdb OR azure.enterpriseknowledgegraph OR azure.eventgrid OR azure.eventhub OR azure.hdinsight OR azure.insights OR azure.iotcentral OR azure.keyvault OR azure.kusto OR azure.logic OR azure.machinelearningservices OR azure.managedidentity OR azure.operationalinsights OR azure.operationsmanagement OR azure.peering OR azure.relay OR azure.resourcegroup OR azure.resources OR azure.search OR azure.security OR azure.servicebus OR azure.servicefabric OR azure.streamanalytics OR azure.subscription OR azure.synapse)"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
  .timestamp = .custom.time
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.category == null {
  if (source = .custom.category; source != null) {
    .custom.evt.category = source
  }
}
del(.custom.category)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.name == null {
  if (source = .custom.operationName; source != null) {
    .custom.evt.name = source
  }
}
del(.custom.operationName)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.outcome == null {
  if (source = .custom.resultType; source != null) {
    .custom.evt.outcome = source
  }
}
del(.custom.resultType)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.callerIpAddress; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.callerIpAddress)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.id == null {
  if (source = .custom.identity.authorization.evidence.principalId; source != null) {
    .custom.usr.id = source
  }
}
del(.custom.identity.authorization.evidence.principalId)
'''

[[transforms.processing.logs]]
name = "azure_web"
filter.type = "datadog_search"
filter.source = "source:azure.web"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
  .timestamp = .custom.time
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.category == null {
  if (source = .custom.category; source != null) {
    .custom.evt.category = source
  }
}
del(.custom.category)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.name == null {
  if (source = .custom.operationName; source != null) {
    .custom.evt.name = source
  }
}
del(.custom.operationName)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.outcome == null {
  if (source = .custom.resultType; source != null) {
    .custom.evt.outcome = source
  }
}
del(.custom.resultType)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.callerIpAddress; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.callerIpAddress)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.id == null {
  if (source = .custom.identity.authorization.evidence.principalId; source != null) {
    .custom.usr.id = source
  }
}
del(.custom.identity.authorization.evidence.principalId)
'''

[[transforms.processing.logs]]
name = "azure_storage"
filter.type = "datadog_search"
filter.source = "source:azure.storage"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
  .timestamp = .custom.time
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.category == null {
  if (source = .custom.category; source != null) {
    .custom.evt.category = source
  }
}
del(.custom.category)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.name == null {
  if (source = .custom.operationName; source != null) {
    .custom.evt.name = source
  }
}
del(.custom.operationName)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.outcome == null {
  if (source = .custom.resultType; source != null) {
    .custom.evt.outcome = source
  }
}
del(.custom.resultType)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.callerIpAddress; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.callerIpAddress)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.id == null {
  if (source = .custom.identity.authorization.evidence.principalId; source != null) {
    .custom.usr.id = source
  }
}
del(.custom.identity.authorization.evidence.principalId)
'''

[[transforms.processing.logs]]
name = "azure_network"
filter.type = "datadog_search"
filter.source = "source:azure.network"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
  .timestamp = .custom.time
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.properties.eventProperties.title) {
  .message = .custom.properties.eventProperties.title
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.category == null {
  if (source = .custom.category; source != null) {
    .custom.evt.category = source
  }
}
del(.custom.category)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.name == null {
  if (source = .custom.operationName; source != null) {
    .custom.evt.name = source
  }
}
del(.custom.operationName)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.outcome == null {
  if (source = .custom.resultType; source != null) {
    .custom.evt.outcome = source
  }
}
del(.custom.resultType)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.callerIpAddress; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.callerIpAddress)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.id == null {
  if (source = .custom.identity.authorization.evidence.principalId; source != null) {
    .custom.usr.id = source
  }
}
del(.custom.identity.authorization.evidence.principalId)
'''

[[transforms.processing.logs]]
name = "azure_compute"
filter.type = "datadog_search"
filter.source = "source:azure.compute"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
  .timestamp = .custom.time
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.properties.eventProperties.title) {
  .message = .custom.properties.eventProperties.title
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.category == null {
  if (source = .custom.category; source != null) {
    .custom.evt.category = source
  }
}
del(.custom.category)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.name == null {
  if (source = .custom.operationName; source != null) {
    .custom.evt.name = source
  }
}
del(.custom.operationName)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.outcome == null {
  if (source = .custom.resultType; source != null) {
    .custom.evt.outcome = source
  }
}
del(.custom.resultType)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.callerIpAddress; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.callerIpAddress)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.id == null {
  if (source = .custom.identity.authorization.evidence.principalId; source != null) {
    .custom.usr.id = source
  }
}
del(.custom.identity.authorization.evidence.principalId)
'''

[[transforms.processing.logs]]
name = "etcd"
filter.type = "datadog_search"
filter.source = "source:etcd"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_date} %{word: level} \\| (%{notSpace:package}: )?%{data:message}",
  ],
  aliases: {
    "etcd": "%{_date} %{word: level} \\| (%{notSpace:package}: )?%{data:message}",
    "_date": "%{date(\"yyyy-MM-dd HH:mm:ss.SSSSSS\"):date}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
  .timestamp = .custom.date
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
  .message = .custom.message
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "glog_pipeline"
filter.type = "datadog_search"
filter.source = "source:(admission-webhook OR api-server OR cert-manager-acmesolver OR cert-manager-cainjector OR cert-manager-controller OR cert-manager-webhook OR cluster-proportional-autoscaler-amd64 OR hyperkube OR ip-masq-agent OR k8s-prometheus-adapter-amd64 OR kube-apiserver OR kube-controller-manager OR kube-proxy OR kube-state-metrics OR metacontroller OR metrics-server-amd64 OR prometheus-operator OR vpa-admission-controller OR vpa-recommender OR vpa-updater)"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{regex(\"[IWEF]\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}",
  ],
  aliases: {
    "glog_rule": "%{regex(\"[IWEF]\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
  .message = .custom.msg
}
'''

[[transforms.processing.logs]]
name = "auth0"
filter.type = "datadog_search"
filter.source = "source:auth0"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.data.date) {
  .timestamp = .custom.data.date
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.data.ip; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.data.ip)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.http.useragent == null {
  if (source = .custom.data.user_agent; source != null) {
    .custom.http.useragent = source
  }
}
del(.custom.data.user_agent)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent, mode: "fast"); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.id == null {
  if (source = .custom.data.user_name; source != null) {
    .custom.usr.id = source
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.name == null {
  if (source = .custom.data.user_name; source != null) {
    .custom.usr.name = source
  }
}
del(.custom.data.user_name)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.email == null {
  if (source = .custom.data.details.request.auth.user.email; source != null) {
    .custom.usr.email = source
  }
}
del(.custom.data.details.request.auth.user.email)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
lookup = {
  "admin_update_launch" : "auth0_update_launched",
  "api_limit" : "rate_limit_on_the_authentication_api",
  "cls" : "code_or_link_sent",
  "coff" : "connector_offline",
  "con" : "connector_online",
  "cs" : "code_sent",
  "depnote" : "deprecation_notice",
  "du" : "deleted_user",
  "f" : "failed_login",
  "fapi" : "failed_api_operation",
  "fc" : "failed_by_connector",
  "fce" : "failed_change_email",
  "fco" : "failed_by_cors",
  "fcoa" : "failed_cross-origin_authentication",
  "fcp" : "failed_change_password",
  "fcph" : "failed_post_change_password_hook",
  "fcpn" : "failed_change_phone_number",
  "fcpr" : "failed_change_password_request",
  "fcpro" : "failed_connector_provisioning",
  "fcu" : "failed_change_username",
  "fd" : "failed_delegation",
  "fdeac" : "failed_device_activation",
  "fdeaz" : "failed_device_authorization_request",
  "fdecc" : "user_canceled_device_confirmation",
  "fdu" : "failed_user_deletion",
  "feacft" : "failed_exchange_authorization_code_for_access_token",
  "feccft" : "failed_exchange_access_token_for_client_credentials_grant",
  "fede" : "failed_exchange_device_code_for_access_token",
  "fens" : "failed_exchange_for_native_social_login",
  "feoobft" : "failed_exchange_password_and_oob_challenge_for_access_token",
  "feotpft" : "failed_exchange_password_and_otp_challege_for_access_token",
  "fepft" : "failed_exchange_password_for_access_token",
  "fercft" : "failed_exchange_password_and_mfa_recovery_code_for_access_token",
  "fertft" : "failed_exchange_refresh_token_for_access_token",
  "flo" : "failed_logout",
  "fn" : "failed_sending_notification",
  "fp" : "failed_login_incorrect_password",
  "fs" : "failed_signup",
  "fsa" : "failed_silent_auth",
  "fu" : "failed_login_invalid_email_or_username",
  "fui" : "failed_users_import",
  "fv" : "failed_verification_email",
  "fvr" : "failed_verification_email_request",
  "gd_auth_failed" : "multifactor_auth_failed",
  "gd_auth_rejected" : "multifactor_auth_rejected",
  "gd_auth_succeed" : "multifactor_auth_success",
  "gd_enrollment_complete" : "multifactor_enrollment_complete",
  "gd_module_switch" : "multifactor_module_switch",
  "gd_otp_rate_limit_exceed" : "multifactor_too_many_failures",
  "gd_recovery_failed" : "multifactor_recovery_failed",
  "gd_recovery_rate_limit_exceed" : "multifactor_too_many_recovery_failures",
  "gd_recovery_succeed" : "multifactor_recovery_success",
  "gd_send_pn" : "multifactor_push_notification_sent",
  "gd_send_sms" : "multifactor_sms_sent",
  "gd_start_auth" : "multifactor_auth_started",
  "gd_start_enroll" : "multifactor_enroll_started",
  "gd_tenant_update" : "guardian_tenant_update",
  "gd_unenroll" : "multifactor_unenroll_device_account",
  "gd_update_device_account" : "multifactor_update_device_account",
  "gd_user_delete" : "multifactor_user_delete",
  "limit_delegation" : "too_many_calls_to_delegation",
  "limit_mu" : "blocked_ip_address",
  "limit_wc" : "blocked_account",
  "pwd_leak" : "breached_password",
  "s" : "success_login",
  "sapi" : "success_api_operation",
  "sce" : "success_change_email",
  "scoa" : "success_cross-origin_authentication",
  "scp" : "success_change_password",
  "scph" : "success_post_change_password_hook",
  "scpn" : "success_change_phone_number",
  "scpr" : "success_change_password_request",
  "scu" : "success_change_username",
  "sd" : "success_delegation",
  "sdu" : "success_user_deletion",
  "seacft" : "success_exchange_authorization_code",
  "seccft" : "success_exchange_client_credentials_grant",
  "sede" : "success_exchange_device_code",
  "sens" : "success_exchange_for_native_social_login",
  "seoobft" : "success_exchange_password_and_oob_challenge",
  "seotpft" : "success_exchange_password_and_otp_challege",
  "sepft" : "success_exchange_password",
  "sercft" : "success_exchange_password_and_mfa_recovery_code",
  "sertft" : "success_exchange_refresh_token",
  "slo" : "success_logout",
  "ss" : "success_signup",
  "ssa" : "success_silent_auth",
  "sui" : "success_users_import",
  "sv" : "success_verification_email",
  "svr" : "success_verification_email_request",
  "sys_os_update_end" : "auth0_os_update_ended",
  "sys_os_update_start" : "auth0_os_update_started",
  "sys_update_end" : "auth0_update_ended",
  "sys_update_start" : "auth0_update_started",
  "ublkdu" : "user_login_block_released",
  "w" : "warnings_during_login"
}
if (lookup_value, err = get(lookup, [.custom.data.type]); lookup_value != null) {
  .custom.evt.name = lookup_value
}
if (.custom.evt.name == null) {
  .custom.evt.name = "type_unknown"
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
lookup = {
  "api_limit" : " The maximum number of requests to the Authentication API in given time has reached.",
  "cls" : " Passwordless login code/link has been sent",
  "coff" : " AD/LDAP Connector is offline ",
  "con" : " AD/LDAP Connector is online and working",
  "cs" : " Passwordless login code has been sent",
  "du" : " User has been deleted.",
  "fce" : " Failed to change user email",
  "fco" : " Origin is not in the Allowed Origins list for the specified application",
  "fcpro" : " Failed to provision a AD/LDAP connector",
  "fcu" : " Failed to change username",
  "fd" : " Failed to generate delegation token",
  "fdeac" : " Failed to activate device.",
  "fdeaz" : " Device authorization request failed.",
  "fdecc" : " User did not confirm device.",
  "feacft" : " Failed to exchange authorization code for Access Token",
  "feccft" : " Failed exchange of Access Token for a Client Credentials Grant",
  "fede" : " Failed to exchange Device Code for Access Token",
  "fens" : " Failed exchange for Native Social Login",
  "feoobft" : " Failed exchange of Password and OOB Challenge for Access Token",
  "feotpft" : " Failed exchange of Password and OTP Challenge for Access Token",
  "fepft" : "Failed exchange of Password for Access Token",
  "fercft" : " Failed Exchange of Password and MFA Recovery code for Access Token",
  "fertft" : " Failed Exchange of Refresh Token for Access Token",
  "flo" : " User logout failed",
  "fn" : " Failed to send email notification",
  "fui" : " Failed to import users",
  "fv" : " Failed to send verification email",
  "fvr" : " Failed to process verification email request",
  "gd_auth_failed" : " One-time password authentication failed.",
  "gd_auth_rejected" : " One-time password authentication rejected.",
  "gd_auth_succeed" : " One-time password authentication success.",
  "gd_recovery_failed" : " Multi-factor recovery code failed.",
  "gd_recovery_rate_limit_exceed" : " Multi-factor recovery code has failed too many times.",
  "gd_recovery_succeed" : " Multi-factor recovery code succeeded authorization.",
  "gd_send_pn" : " Push notification for MFA sent successfully sent.",
  "gd_send_sms" : " SMS for MFA sent successfully sent.",
  "gd_start_auth" : " Second factor authentication event started for MFA.",
  "gd_start_enroll" : " Multi-factor authentication enroll has started.",
  "gd_unenroll" : " Device used for second factor authentication has been unenrolled.",
  "gd_update_device_account" : " Device used for second factor authentication has been updated.",
  "gd_user_delete" : " Deleted multi-factor user account.",
  "limit_delegation" : " Rate limit exceeded to /delegation endpoint",
  "limit_mu" : " An IP address is blocked with 100 failed login attempts using different usernames all with incorrect passwords in 24 hours or 50 sign-up attempts per minute from the same IP address.",
  "limit_wc" : " An IP address is blocked with 10 failed login attempts into a single account from the same IP address.",
  "pwd_leak" : " Someone behind the IP address: ip attempted to login with a leaked password.",
  "s" : " Successful login event.",
  "sdu" : " User successfully deleted",
  "seacft" : " Successful exchange of authorization code for Access Token",
  "seccft" : " Successful exchange of Access Token for a Client Credentials Grant",
  "sede" : " Successful exchange of device code for Access Token",
  "sens" : " Native Social Login",
  "seoobft" : " Successful exchange of Password and OOB Challenge for Access Token",
  "seotpft" : " Successful exchange of Password and OTP Challenge for Access Token",
  "sepft" : " Successful exchange of Password for Access Token",
  "sercft" : " Successful exchange of Password and MFA Recovery code for Access Token",
  "sertft" : " Successful exchange of Refresh Token for Access Token",
  "slo" : " User successfully logged out",
  "sui" : " Successfully imported users",
  "ublkdu" : " User block setup by anomaly detection has been released"
}
if (lookup_value, err = get(lookup, [.custom.data.type]); lookup_value != null) {
  .custom.message = lookup_value
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
  .message = .custom.message
} else if exists(.custom.data.description) {
  .message = .custom.data.description
} else if exists(.custom.evt.name) {
  .message = .custom.evt.name
}
'''

[[transforms.processing.logs]]
name = "kube_scheduler_glog"
filter.type = "datadog_search"
filter.source = "source:(kube_scheduler OR kube-scheduler)"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}",
  ],
  aliases: {
    "kube_scheduler": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
  .timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
  .message = .custom.msg
}
'''

[[transforms.processing.logs]]
name = "aws_ecs-agent"
filter.type = "datadog_search"
filter.source = "source:amazon-ecs-agent"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
  .custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
  .custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
  .custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
  .custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
  .timestamp = .custom.time
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
  .message = .custom.msg
}
'''

[[transforms.processing.logs]]
name = "nodejs"
filter.type = "datadog_search"
filter.source = "source:nodejs"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
  .message = .custom.msg
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)(?>\\[%{data::keyvalue}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):time}\\]\\s+\\[%{word:level}\\] %{data::keyvalue}",
    "(?s)%{word:http.method} %{notSpace:http.url} %{number:http.status_code} (-|%{number:network.bytes_written}) - (%{number:duration:scale(1000000)} ms|%{number:duration:scale(1000000000)} s)",
    "(?s)%{ipOrHost:network.client.ip} %{notSpace:http.ident:nullIf(\"-\")} %{notSpace:http.auth:nullIf(\"-\")} \\[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}\\] \"(?>%{word:http.method} |)%{notSpace:http.url}(?> HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}|)\" %{number:http.status_code} (?>%{number:network.bytes_written}|-) \"%{notSpace:http.referer}\" \"%{regex(\"[^\\\\\\\"]*\"):http.useragent}\".*",
    "(?s)(?>\\[%{data::keyvalue}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?%{data::json}",
  ],
  aliases: {
    "log4js_format": "(?>\\[%{data::keyvalue}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):time}\\]\\s+\\[%{word:level}\\] %{data::keyvalue}",
    "web_access_morgan": "%{word:http.method} %{notSpace:http.url} %{number:http.status_code} (-|%{number:network.bytes_written}) - (%{number:duration:scale(1000000)} ms|%{number:duration:scale(1000000000)} s)",
    "morgan_combined": "%{ipOrHost:network.client.ip} %{notSpace:http.ident:nullIf(\"-\")} %{notSpace:http.auth:nullIf(\"-\")} \\[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}\\] \"(?>%{word:http.method} |)%{notSpace:http.url}(?> HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}|)\" %{number:http.status_code} (?>%{number:network.bytes_written}|-) \"%{notSpace:http.referer}\" \"%{regex(\"[^\\\\\\\"]*\"):http.useragent}\".*",
    "fallback": "(?>\\[%{data::keyvalue}dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?%{data::json}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
  .timestamp = .custom.time
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
  .custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
  .custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
  .custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
  .custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@level:10") {
  .custom.bunyan_level = "trace"
} else if match_datadog_query(., "@level:20") {
  .custom.bunyan_level = "debug"
} else if match_datadog_query(., "@level:30") {
  .custom.bunyan_level = "info"
} else if match_datadog_query(., "@level:40") {
  .custom.bunyan_level = "warning"
} else if match_datadog_query(., "@level:50") {
  .custom.bunyan_level = "error"
} else if match_datadog_query(., "@level:60") {
  .custom.bunyan_level = "fatal"
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.bunyan_level) ?? string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
  .trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  details.queryString = del(details.query)
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent, mode: "fast"); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.dd.env != null {
  if (tag, err = "env:" + to_string(.custom.dd.env); err == null) {
    .tags, err = push(.tags, tag)
  }
}
del(.custom.dd.env)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.dd.version != null {
  if (tag, err = "version:" + to_string(.custom.dd.version); err == null) {
    .tags, err = push(.tags, tag)
  }
}
del(.custom.dd.version)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
  .service = .custom.dd.service
}
'''

[[transforms.processing.logs]]
name = "postgresql"
filter.type = "datadog_search"
filter.source = "source:postgresql"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_prefix} %{_severity}:\\s+duration:\\s+%{_duration}\\s+ms\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
    "(?s)%{_prefix} %{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
    "(?s)(%{_timestamp}|%{_timestamp_ms} \\[%{_proc_id}])\\s+%{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
  ],
  aliases: {
    "suggested_format_with_duration": "%{_prefix} %{_severity}:\\s+duration:\\s+%{_duration}\\s+ms\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
    "suggested_format": "%{_prefix} %{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
    "default_format": "(%{_timestamp}|%{_timestamp_ms} \\[%{_proc_id}])\\s+%{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
    "_timestamp": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):db.date}",
    "_timestamp_ms": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS z\"):db.date}",
    "_database": "%{notSpace:db.instance}",
    "_raw_query": "%{data:db.statement}",
    "_duration": "%{numberExt:duration}",
    "_severity": "%{notSpace:db.severity}",
    "_user": "%{notSpace:db.user}",
    "_client_ip": "%{notSpace:network.client.ip}",
    "_proc_id": "%{notSpace:postgres.proc_id}",
    "_session_id": "%{notSpace:postgres.session_id}",
    "_app": "%{notSpace:postgres.appname}",
    "_prefix": "%{_timestamp_ms} \\[%{_proc_id}\\] %{_database} %{_app} %{_user} %{_client_ip} %{_session_id}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.db.statement,
  patterns: [
    "(?s)%{word:db.operation} .*",
  ],
  aliases: {
    "extract_operation": "%{word:db.operation} .*"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
  .timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.db == null {
  if .custom.db.instance != null {
    if (tag, err = "db:" + to_string(.custom.db.instance); err == null) {
      .tags, err = push(.tags, tag)
    }
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.duration * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs]]
name = "cassandra"
filter.type = "datadog_search"
filter.source = "source:cassandra"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_prefix} %{regex(\"Compacting\"):db.operation}.* %{_keyspace}\\/%{_table}:%{data:partition_key} \\(%{_bytes} bytes\\)",
    "(?s)%{_prefix} %{regex(\"Flushing\"):db.operation}.*\\(Keyspace='%{_keyspace}', ColumnFamily='%{_table}'\\) %{data}: %{_onheap_total}\\/%{_offheap_total}, live: %{_onheap_live}\\/%{_offheap_live}, flushing: %{_onheap_flush}\\/%{_offheap_flush}, this: %{_onheap_this}\\/%{_offheap_this}",
    "(?s)%{_prefix} %{regex(\"Enqueuing\"):db.operation}.* of %{_keyspace}: %{_onheap_bytes}%{data} \\(%{_onheap_pct}%\\) on-heap, %{_offheap_bytes} \\(%{_offheap_pct}%\\).*",
    "(?s)%{_prefix} %{regex(\"Writing\"):db.operation}.*-%{_keyspace}%{data}\\(%{number:cassandra.bytes:scale(1000000)}%{data}, %{integer:cassandra.ops} ops, %{_onheap_pct}%\\/%{_offheap_pct}.*",
    "(?s)%{_prefix} Completed %{regex(\"flushing\"):db.operation} %{_sstable} \\(%{number:cassandra.bytes_kb}KiB\\) for commitlog %{data:commitlog}",
    "(?s)%{_prefix}\\s+%{regex(\"Compacted\"):db.operation}.* to \\[%{_sstable}\\].\\s+%{notSpace:cassandra.bytes_in} bytes to %{notSpace:cassandra.bytes_out} \\(\\~%{integer:cassandra.percent_of_orig}% of original\\) in %{notSpace:cassandra.duration_ms}ms = %{number:cassandra.speed_mb}MB/s.\\s+%{notSpace:cassandra.pkeys_in} total partitions merged to %{notSpace:cassandra.pkeys_out}\\.\\s+Partition merge counts were %{data:cassandra.merge_cnt}",
    "(?s)%{_prefix} G.* %{integer:duration:scale(1000000)}ms. %{data}: %{integer:cassandra.eden.orig_bytes} -> %{integer:cassandra.eden.new_bytes}; %{data}: %{integer:cassandra.oldgen.orig_bytes} -> %{integer:cassandra.oldgen.new_bytes};.*",
    "(?s)%{_prefix} %{word:cassandra.pool}\\s*(?>%{integer:cassandra.cache_used}\\s*%{integer:cassandra.cache_size}\\s*all|%{integer:cassandra.threads.active}\\s*%{integer:cassandra.threads.pending}\\s*%{integer:cassandra.threads.completed}\\s*%{integer:cassandra.threads.blocked}\\s*%{integer:cassandra.threads.all_time_blocked}|%{integer:cassandra.threads.active}\\s*%{integer:cassanadra.threads.pending})",
    "(?s)%{_prefix} %{integer:db.operations} operations were slow in the last %{integer:elapsed_time:scale(1000000)} msecs:\\n%{data:db.slow_statements:array(\"\", \"\\\\n\")}",
    "(?s)%{_prefix} %{data:msg}",
  ],
  aliases: {
    "cassandra_compaction_key": "%{_prefix} %{regex(\"Compacting\"):db.operation}.* %{_keyspace}\\/%{_table}:%{data:partition_key} \\(%{_bytes} bytes\\)",
    "cassandra_pool_cleaner": "%{_prefix} %{regex(\"Flushing\"):db.operation}.*\\(Keyspace='%{_keyspace}', ColumnFamily='%{_table}'\\) %{data}: %{_onheap_total}\\/%{_offheap_total}, live: %{_onheap_live}\\/%{_offheap_live}, flushing: %{_onheap_flush}\\/%{_offheap_flush}, this: %{_onheap_this}\\/%{_offheap_this}",
    "cassandra_pool_cleaner2": "%{_prefix} %{regex(\"Enqueuing\"):db.operation}.* of %{_keyspace}: %{_onheap_bytes}%{data} \\(%{_onheap_pct}%\\) on-heap, %{_offheap_bytes} \\(%{_offheap_pct}%\\).*",
    "cassandra_table_flush": "%{_prefix} %{regex(\"Writing\"):db.operation}.*-%{_keyspace}%{data}\\(%{number:cassandra.bytes:scale(1000000)}%{data}, %{integer:cassandra.ops} ops, %{_onheap_pct}%\\/%{_offheap_pct}.*",
    "cassandra_mem_flush": "%{_prefix} Completed %{regex(\"flushing\"):db.operation} %{_sstable} \\(%{number:cassandra.bytes_kb}KiB\\) for commitlog %{data:commitlog}",
    "cassandra_compaction": "%{_prefix}\\s+%{regex(\"Compacted\"):db.operation}.* to \\[%{_sstable}\\].\\s+%{notSpace:cassandra.bytes_in} bytes to %{notSpace:cassandra.bytes_out} \\(\\~%{integer:cassandra.percent_of_orig}% of original\\) in %{notSpace:cassandra.duration_ms}ms = %{number:cassandra.speed_mb}MB/s.\\s+%{notSpace:cassandra.pkeys_in} total partitions merged to %{notSpace:cassandra.pkeys_out}\\.\\s+Partition merge counts were %{data:cassandra.merge_cnt}",
    "cassandra_gc_format": "%{_prefix} G.* %{integer:duration:scale(1000000)}ms. %{data}: %{integer:cassandra.eden.orig_bytes} -> %{integer:cassandra.eden.new_bytes}; %{data}: %{integer:cassandra.oldgen.orig_bytes} -> %{integer:cassandra.oldgen.new_bytes};.*",
    "cassandra_thread_pending": "%{_prefix} %{word:cassandra.pool}\\s*(?>%{integer:cassandra.cache_used}\\s*%{integer:cassandra.cache_size}\\s*all|%{integer:cassandra.threads.active}\\s*%{integer:cassandra.threads.pending}\\s*%{integer:cassandra.threads.completed}\\s*%{integer:cassandra.threads.blocked}\\s*%{integer:cassandra.threads.all_time_blocked}|%{integer:cassandra.threads.active}\\s*%{integer:cassanadra.threads.pending})",
    "cassandra_slow_statements": "%{_prefix} %{integer:db.operations} operations were slow in the last %{integer:elapsed_time:scale(1000000)} msecs:\\n%{data:db.slow_statements:array(\"\", \"\\\\n\")}",
    "cassandra_fallback_parser": "%{_prefix} %{data:msg}",
    "_level": "%{word:db.severity}",
    "_thread_name": "%{notSpace:logger.thread_name}",
    "_thread_id": "%{integer:logger.thread_id}",
    "_logger_name": "%{notSpace:logger.name}",
    "_table": "%{word:db.table}",
    "_sstable": "%{notSpace:cassandra.sstable}",
    "_bytes": "%{integer:cassandra.bytes}",
    "_keyspace": "%{word:cassandra.keyspace}",
    "_onheap_total": "%{number:cassandra.onheap.total}",
    "_onheap_live": "%{number:cassandra.onheap.live}",
    "_onheap_flush": "%{number:cassandra.onheap.flush}",
    "_onheap_this": "%{number:cassandra.onheap.this}",
    "_onheap_bytes": "%{integer:cassandra.onheap.bytes}",
    "_onheap_pct": "%{integer:cassandra.onheap.percent}",
    "_offheap_total": "%{number:cassandra.offheap.total}",
    "_offheap_live": "%{number:cassandra.offheap.live}",
    "_offheap_flush": "%{number:cassandra.offheap.flush}",
    "_offheap_this": "%{number:cassandra.offheap.this}",
    "_offheap_bytes": "%{integer:cassandra.offheap.bytes}",
    "_offheap_pct": "%{integer:cassandra.offheap.percent}",
    "_default_prefix": "%{_level}\\s+\\[(%{_thread_name}:%{_thread_id}|%{_thread_name})\\]\\s+%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):db.date}\\s+%{word:filename}.java:%{integer:lineno} -",
    "_suggested_prefix": "%{date(\"yyyy-MM-dd HH:mm:ss\"):db.date} \\[(%{_thread_name}:%{_thread_id}|%{_thread_name})\\] %{_level} %{_logger_name}\\s+-",
    "_prefix": "(?>%{_default_prefix}|%{_suggested_prefix})"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
  .timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "apache_httpd"
filter.type = "datadog_search"
filter.source = "source:httpd"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
    "(?s)%{access.common} \"%{_referer}\" \"%{_user_agent}\"",
  ],
  aliases: {
    "access.common": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
    "access.combined": "%{access.common} \"%{_referer}\" \"%{_user_agent}\"",
    "_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
    "_bytes_written": "%{integer:network.bytes_written}",
    "_client_ip": "%{ipOrHost:network.client.ip}",
    "_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
    "_url": "%{notSpace:http.url}",
    "_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
    "_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
    "_referer": "%{notSpace:http.referer}",
    "_status_code": "%{integer:http.status_code}",
    "_method": "%{word:http.method}",
    "_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent, mode: "fast"); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  details.queryString = del(details.query)
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
  .timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if match_datadog_query(., "@http.status_code:[200 TO 299]") {
  .custom.http.status_category = "OK"
} else if match_datadog_query(., "@http.status_code:[300 TO 399]") {
  .custom.http.status_category = "notice"
} else if match_datadog_query(., "@http.status_code:[400 TO 499]") {
  .custom.http.status_category = "warning"
} else if match_datadog_query(., "@http.status_code:[500 TO 599]") {
  .custom.http.status_category = "error"
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs]]
name = "azure_recovery_services"
filter.type = "datadog_search"
filter.source = "source:azure.recoveryservices"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
  .timestamp = .custom.time
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.category == null {
  if (source = .custom.category; source != null) {
    .custom.evt.category = source
  }
}
del(.custom.category)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.name == null {
  if (source = .custom.operationName; source != null) {
    .custom.evt.name = source
  }
}
del(.custom.operationName)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.evt.outcome == null {
  if (source = .custom.resultType; source != null) {
    .custom.evt.outcome = source
  }
}
del(.custom.resultType)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.network.client.ip == null {
  if (source = .custom.callerIpAddress; source != null) {
    .custom.network.client.ip = source
  }
}
del(.custom.callerIpAddress)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (result, err = .custom.durationMs * 1000000; err == null) {
  .custom.duration = result
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.usr.id == null {
  if (source = .custom.identity.authorization.evidence.principalId; source != null) {
    .custom.usr.id = source
  }
}
del(.custom.identity.authorization.evidence.principalId)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.entity_name == null {
  if (source = .custom.properties.Entity_Name; source != null) {
    .custom.entity_name = source
  }
}
del(.custom.properties.Entity_Name)
'''

[[transforms.processing.logs]]
name = "c"
filter.type = "datadog_search"
filter.source = "source:csharp"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.logger.name == null {
  if (source = .custom.logger; source != null) {
    .custom.logger.name = source
  }
}
del(.custom.logger)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.logger.thread_name == null {
  if (source = .custom.thread; source != null) {
    .custom.logger.thread_name = source
  }
}
del(.custom.thread)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.error.stack == null {
  if (source = .custom.exception; source != null) {
    .custom.error.stack = source
  } else if (source = .custom.stack_trace; source != null) {
    .custom.error.stack = source
  }
}
del(.custom.exception)
del(.custom.stack_trace)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.message,
  patterns: [
    "(?s)%{_date} %{_status} \\[%{_thread_name}\\] %{_logger_name} %{_method}:%{_line} - %{data:message}((\\n|\\t)%{data:error.stack})?",
    "(?s)(%{_date}|%{_date_ms}) %{_status}\\s+%{data:message}((\\n|\\t)%{data:error.stack})?",
    "(?s)%{date(\"yyyy-MM-dd HH:mm:ss.SSS Z\"):date} \\[%{_status}\\] %{data::keyvalue(\": \")}",
    "(?s)(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSSSS\"):date})\\|%{_status}\\|%{notSpace:logger.name}\\|\\{%{data::keyvalue}\\}.*",
    "(?s)%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):date} \\[%{number}\\] %{_status}\\s+%{notSpace:logger.name} \\[%{notSpace}\\] \\{%{data:Properties:keyvalue(\"=\")}\\}.*",
  ],
  aliases: {
    "recommended_format": "%{_date} %{_status} \\[%{_thread_name}\\] %{_logger_name} %{_method}:%{_line} - %{data:message}((\\n|\\t)%{data:error.stack})?",
    "default_parser": "(%{_date}|%{_date_ms}) %{_status}\\s+%{data:message}((\\n|\\t)%{data:error.stack})?",
    "serilog_format": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS Z\"):date} \\[%{_status}\\] %{data::keyvalue(\": \")}",
    "Nlog_format": "(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSSSS\"):date})\\|%{_status}\\|%{notSpace:logger.name}\\|\\{%{data::keyvalue}\\}.*",
    "log4net_format": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):date} \\[%{number}\\] %{_status}\\s+%{notSpace:logger.name} \\[%{notSpace}\\] \\{%{data:Properties:keyvalue(\"=\")}\\}.*",
    "_date": "(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date})",
    "_date_ms": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):date}",
    "_status": "%{word:level}",
    "_thread_name": "%{notSpace:logger.thread_name}",
    "_logger_name": "%{notSpace:logger.name}",
    "_line": "%{integer:line}",
    "_method": "%{notSpace:method}"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
  .timestamp = .custom.date
} else if exists(.custom.time) {
  .timestamp = .custom.time
} else if exists(.custom.@t) {
  .timestamp = .custom.@t
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? string(.custom.Level) ?? string(.custom.@l) ?? ""
status = downcase(status)
if status == "" {
 .status = 6
} else {
  if starts_with(status, "f") || starts_with(status, "emerg") {
    .status = 0
  } else if starts_with(status, "a") {
    .status = 1
  } else if starts_with(status, "c") {
    .status = 2
  } else if starts_with(status, "e") {
    .status = 3
  } else if starts_with(status, "w") {
    .status = 4
  } else if starts_with(status, "n") {
    .status = 5
  } else if starts_with(status, "i") {
    .status = 6
  } else if starts_with(status, "d") || starts_with(status, "trace") || starts_with(status, "verbose") {
    .status = 7
  } else if starts_with(status, "o") || starts_with(status, "s") || status == "ok" || status == "success" {
    .status = 8
  }
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .custom.error.stack,
  patterns: [
    "(?s)%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*",
  ],
  aliases: {
    "error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"
  }
)
if (err == null) {
  .custom, err = merge(.custom, custom, deep: true)
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
  .trace_id = .custom.dd.trace_id
} else if exists(.custom.dd_trace_id) {
  .trace_id = .custom.dd_trace_id
} else if exists(.custom.Properties.dd.trace_id) {
  .trace_id = .custom.Properties.dd.trace_id
} else if exists(.custom.properties.dd.trace_id) {
  .trace_id = .custom.properties.dd.trace_id
} else if exists(.custom.Properties.dd_trace_id) {
  .trace_id = .custom.Properties.dd_trace_id
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.RenderedMessage) {
  .message = .custom.RenderedMessage
} else if exists(.custom.@m) {
  .message = .custom.@m
} else if exists(.custom.@mt) {
  .message = .custom.@mt
} else if exists(.custom.message) {
  .message = .custom.message
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.dd.env != null {
  if (tag, err = "env:" + to_string(.custom.dd.env); err == null) {
    .tags, err = push(.tags, tag)
  }
} else if .custom.dd_env != null {
  if (tag, err = "env:" + to_string(.custom.dd_env); err == null) {
    .tags = push(.tags, tag)
  }
} else if .custom.Properties.dd.env != null {
  if (tag, err = "env:" + to_string(.custom.Properties.dd.env); err == null) {
    .tags = push(.tags, tag)
  }
} else if .custom.properties.dd.env != null {
  if (tag, err = "env:" + to_string(.custom.properties.dd.env); err == null) {
    .tags = push(.tags, tag)
  }
} else if .custom.Properties.dd_env != null {
  if (tag, err = "env:" + to_string(.custom.Properties.dd_env); err == null) {
    .tags = push(.tags, tag)
  }
}
del(.custom.dd.env)
del(.custom.dd_env)
del(.custom.Properties.dd.env)
del(.custom.properties.dd.env)
del(.custom.Properties.dd_env)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if .custom.dd.version != null {
  if (tag, err = "version:" + to_string(.custom.dd.version); err == null) {
    .tags, err = push(.tags, tag)
  }
} else if .custom.dd_version != null {
  if (tag, err = "version:" + to_string(.custom.dd_version); err == null) {
    .tags = push(.tags, tag)
  }
} else if .custom.Properties.dd.version != null {
  if (tag, err = "version:" + to_string(.custom.Properties.dd.version); err == null) {
    .tags = push(.tags, tag)
  }
} else if .custom.properties.dd.version != null {
  if (tag, err = "version:" + to_string(.custom.properties.dd.version); err == null) {
    .tags = push(.tags, tag)
  }
} else if .custom.Properties.dd_version != null {
  if (tag, err = "version:" + to_string(.custom.Properties.dd_version); err == null) {
    .tags = push(.tags, tag)
  }
}
del(.custom.dd.version)
del(.custom.dd_version)
del(.custom.Properties.dd.version)
del(.custom.properties.dd.version)
del(.custom.Properties.dd_version)
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
  .service = .custom.dd.service
} else if exists(.custom.dd_service) {
  .service = .custom.dd_service
} else if exists(.custom.Properties.dd.service) {
  .service = .custom.Properties.dd.service
} else if exists(.custom.properties.dd.service) {
  .service = .custom.properties.dd.service
} else if exists(.custom.Properties.dd_service) {
  .service = .custom.Properties.dd_service
}
'''

[[transforms.processing.logs]]
name = "web_browser_logs"
filter.type = "datadog_search"
filter.source = "source:browser"

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.http.url); err == null) {
  details.queryString = del(details.query)
  .custom.http.url_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_user_agent(.custom.http.useragent, mode: "fast"); err == null) {
  .custom.http.useragent_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.view.url); err == null) {
  details.queryString = del(details.query)
  .custom.view.url_details = details
}
'''

[[transforms.processing.logs.transforms]]
type = "remap"
source = '''
if (details, err = parse_url(.custom.view.referrer); err == null) {
  details.queryString = del(details.query)
  .custom.view.referrer_details = details
}
'''

##
## Sinks
##

[sinks.prometheus]
type = "prometheus_exporter"
inputs = ["internal_metrics"]
address = "0.0.0.0:9090"

[sinks.blackhole]
type = "blackhole"
print_interval_secs = 0
inputs = ["processing"]
