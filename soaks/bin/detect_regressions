#!/usr/bin/env python3
import numpy as np
import pandas as pd
import scipy.stats
import argparse
import math
import glob
import os
import sys
import gc
import common

np.seterr(all='raise')

parser = argparse.ArgumentParser(description='t-test experiments with Welch method')
parser.add_argument('--capture-dir', type=str, help='the directory to search for capture csv files')
parser.add_argument('--erratic-soaks', type=str, default='', help='a comma separated list of known-erratic experiments, NOT TO BE USED LIGHTLY')
parser.add_argument('--mean-drift-percentage', type=float, default=5, help='the percentage of mean drift we allow in an experiment, expressed as a value from 0 to 100, default 5')
parser.add_argument('--p-value', type=float, default=0.05, help='the p-value for comparing with t-test results, the smaller the more certain')
parser.add_argument('--warmup-seconds', type=int, default=30, help='the number of seconds to treat as warmup')
args = parser.parse_args()

known_erratic_soaks = args.erratic_soaks.split(',')

bytes_written = pd.concat(common.compute_throughput(
    common.open_captures(args.capture_dir,
                         'bytes_written',
                         unwanted_labels=['metric_name', 'metric_kind', 'target'])))
# Skip past warmup seconds samples, allowing for vector warmup to not factor
# into judgement. Also, filter any zero samples as these are not interesting for
# the purposes of analysis.
bytes_written = bytes_written[(bytes_written.fetch_index > args.warmup_seconds) &
                              (bytes_written.throughput > 0.0)]

results = []
for exp in bytes_written.experiment.unique():
    baseline = bytes_written.loc[(bytes_written.experiment == exp) & (bytes_written.variant == 'baseline')]
    comparison = bytes_written.loc[(bytes_written.experiment == exp) & (bytes_written.variant == 'comparison')]

    baseline_mean = baseline.throughput.mean()
    baseline_stdev = baseline.throughput.std()
    comparison_mean = comparison.throughput.mean()
    comparison_stdev = comparison.throughput.std()
    diff =  comparison_mean - baseline_mean
    percent_change = round(((comparison_mean - baseline_mean) / baseline_mean) * 100, 2)

    baseline_outliers = common.total_outliers(baseline)
    comparison_outliers = common.total_outliers(comparison)

    # The t-test here is calculating whether the expected mean of our two
    # distributions is equal, or, put another way, whether the samples we have
    # here are from identical distributions. The higher the returned p-value by
    # ttest_ind the more likely it is that the samples _do_ have the same
    # expected mean.
    #
    # If the p-value is below our threshold then it is _unlikely_ that the two
    # samples actually have the same mean -- are from the same distribution --
    # and so there's some statistically interesting difference between the two
    # samples. For our purposes here that implies that performance has changed.
    res = scipy.stats.ttest_ind_from_stats(baseline_mean,
                                           baseline_stdev,
                                           len(baseline),
                                           comparison_mean,
                                           comparison_stdev,
                                           len(comparison),
                                           equal_var=False)
    results.append({'experiment': exp,
                    'Δ mean': diff.mean(),
                    'Δ mean %': percent_change,
                    'baseline mean': baseline_mean,
                    'comparison mean': comparison_mean,
                    'p-value': res.pvalue,
                    'declared erratic': exp in known_erratic_soaks
                    })
results = pd.DataFrame.from_records(results)
print("Table of test results:")
print("")
print("")
print(results.to_markdown(index=False, tablefmt='github'))

p_value_violation = results['p-value'] < args.p_value
drift_filter = results['Δ mean %'] < -args.mean_drift_percentage
declared_erratic = results.experiment.isin(known_erratic_soaks)

changes = results[p_value_violation & drift_filter & ~declared_erratic]
print("")
print(f"Table normalized to only show regressions, {args.p_value} p-value threshold, {args.mean_drift_percentage} drift threshold:")
print("")
print(changes.to_markdown(index=False, tablefmt='github'))

if len(changes) > 0:
    print("Regressions detected beyond thresholds.")
    sys.exit(1)
